# Base configuration for SCONE

# Data configuration
data:
  dataset_name: wikitext
  dataset_config_name: wikitext-103-v1
  text_column: text
  max_length: 512
  max_n: 3
  min_freq: 100
  max_f_grams: 1000000

# Model configuration
model:
  base_model_name: gpt2
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  hidden_act: gelu
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 1024
  type_vocab_size: 2
  initializer_range: 0.02
  layer_norm_eps: 1e-12
  use_f_gram_embeddings: true

# Training configuration
training:
  output_dir: ./outputs/scone-base
  num_epochs: 3
  batch_size: 8
  learning_rate: 5e-5
  weight_decay: 0.01
  warmup_steps: 0
  max_grad_norm: 1.0
  logging_steps: 100
  save_steps: 1000
  eval_steps: 1000
  seed: 42

# Inference configuration
inference:
  use_memory_map: true
  batch_size: 32 